# action.yml
name: "LLM in GitHub Actions"
description: "Spin up a vLLM CPU server for any Hugging Face LLM model."

author: "Muhan Li"
branding:
  icon: "message-square"
  color: "blue"

###############
# Inputs & Outputs
###############
inputs:
  model:
    description: "Hugging Face model repo (default Qwen/Qwen3-1.7B)"
    required: false
    default: "Qwen/Qwen3-1.7B"
  vllm_version:
    description: "vLLM git tag (default v0.9.0)"
    required: false
    default: "v0.9.0"
  hf_token:
    description: "Optional Hugging Face access token"
    required: false

outputs:
  logs:
    description: "vLLM server logs"
    value: ${{ steps.show_logs.outputs.log }}

################
# Run the Action
################

runs:
  using: "composite"
  steps:
    #########################################
    # 1. Build vLLM (CPU)
    #########################################

    - name: Checkout vLLM
      uses: actions/checkout@v4
      with:
        repository: "vllm-project/vllm"
        ref: ${{ inputs.vllm_version }}

    - name: Install OS dependencies
      shell: bash
      run: |
        sudo apt-get update -y
        sudo apt-get install -y gcc-12 g++-12 libnuma-dev jq
        sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

    - name: Build vLLM
      shell: bash
      run: |
        pip install --upgrade pip
        pip install "cmake>=3.26" wheel packaging ninja "setuptools-scm>=8" numpy
        pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
        sudo -E VLLM_TARGET_DEVICE=cpu python setup.py install

    - name: Install Python libraries
      shell: bash
      run: pip install openai huggingface_hub[cli]

    - name: Login to Hugging Face
      if: ${{ inputs.hf_token != '' }}
      shell: bash
      env:
        HF: ${{ inputs.hf_token }}
      run: huggingface-cli login --token "$HF"
      continue-on-error: true

    #########################################
    # 2. Run vLLM server
    #########################################

    - name: Run vLLM server
      shell: bash
      run: vllm serve "${{ inputs.model }}" &> vllm.log &

    #########################################
    # 3. Error handling: max seq len exceeded
    #########################################

    - name: Check server status
      id: check_server
      shell: bash
      run: |
        for i in {1..30}; do
          if curl -sSf http://localhost:8000/v1/models > /dev/null; then
            exit 0  # Server started
          fi
          if grep -q "ValueError: The model's max seq len" vllm.log; then
            exit 1  # Error detected
          fi
          sleep 10
        done
        cat vllm.log
      continue-on-error: true

    - name: Parse allowed length
      id: check_maxlen
      if: steps.check_server.outcome != 'success'
      shell: bash
      run: |
        allowed_len=$(grep -m1 -oP "tokens that can be stored in KV cache \\(\\K[0-9]+" vllm.log)
        echo "allowed_len=$allowed_len" >> "$GITHUB_OUTPUT"

    - name: Run vLLM server (retry)
      if: steps.check_server.outcome != 'success'
      shell: bash
      run: |
        pkill -f "vllm.serve" || true
        vllm serve "${{ inputs.model }}" --max-model-len ${{ steps.check_maxlen.outputs.allowed_len }} &> vllm.log &

    #########################################
    # 5. Wait for server to be ready
    #########################################

    - name: Wait until ready (30 min max)
      shell: bash
      run: |
        for i in {1..360}; do
          if curl -sSf http://localhost:8000/v1/models > /dev/null; then
            echo "vLLM server is ready"; exit 0
          fi
          sleep 5
        done
        echo "Timeout: server not ready in 30 min" && exit 1

    #########################################
    # 6. Show logs
    #########################################

    - name: Show logs
      id: show_logs
      shell: bash
      run: |
        echo "log<<EOF" >> "$GITHUB_OUTPUT"
        cat vllm.log    >> "$GITHUB_OUTPUT"
        echo "EOF"      >> "$GITHUB_OUTPUT"
