# action.yml
name: "LLM in GitHub Actions"
description: "Run an LLM model on a vLLM server inside your CI pipeline."

author: "Muhan Li"
branding:
  icon: "message-square"
  color: "red"

###############
# Inputs & Outputs
###############
inputs:
  model:
    description: "Hugging Face model repo (default Qwen/Qwen3-1.7B)"
    required: false
    default: "Qwen/Qwen3-1.7B"
  vllm_version:
    description: "vLLM git tag (default v0.9.0)"
    required: false
    default: "v0.9.0"
  hf_token:
    description: "Optional Hugging Face access token"
    required: false

outputs:
  logs:
    description: "vLLM server logs"
    value: ${{ steps.show_logs.outputs.log }}

################
# Run the Action
################

runs:
  using: "composite"

  steps:
    #########################################
    # 1. Build vLLM (CPU)
    #########################################

    - name: Checkout vLLM
      uses: actions/checkout@v4
      with:
        repository: "vllm-project/vllm"
        ref: ${{ inputs.vllm_version }}
        path: ./.llm-actions

    - name: Install OS dependencies
      shell: bash
      run: |
        sudo apt-get update -y
        sudo apt-get install -y gcc-12 g++-12 libnuma-dev jq
        sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

    - name: Build vLLM
      shell: bash
      run: |
        mkdir ../.llm-actions && mv ./.llm-actions ../.llm-actions/vllm && cd ../.llm-actions/vllm
        pip install --upgrade pip
        pip install "cmake>=3.26" wheel packaging ninja "setuptools-scm>=8" numpy
        pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
        sudo -E VLLM_TARGET_DEVICE=cpu python setup.py install

    - name: Install Python libraries
      shell: bash
      run: pip install openai huggingface_hub[cli]

    - name: Login to Hugging Face
      if: ${{ inputs.hf_token != '' }}
      shell: bash
      env:
        HF: ${{ inputs.hf_token }}
      run: huggingface-cli login --token "$HF"
      continue-on-error: true

    #########################################
    # 2. Run vLLM server
    #########################################

    - name: Run vLLM server
      shell: bash
      run: vllm serve "${{ inputs.model }}" &> ../.llm-actions/vllm.log &

    #########################################
    # 3. Error handling: max seq len exceeded
    #########################################

    - name: Check server status
      id: check_server
      shell: bash
      run: |
        status="timeout"
        for i in {1..30}; do
          if curl -sSf http://localhost:8000/v1/models > /dev/null; then
            status="ready"
            break
          fi
          if grep -q "ValueError: The model's max seq len" ../.llm-actions/vllm.log; then
            status="maxlen"
            break
          fi
          sleep 10
        done
        echo "status=$status" >> "$GITHUB_OUTPUT"
        echo "Server status: $status"

    - name: Retry if max seq len exceeded
      if: steps.check_server.outputs.status == 'maxlen'
      shell: bash
      run: |
        pkill -f "vllm.serve" || true
        len=$(grep -m1 -oP "tokens that can be stored in KV cache \\(\\K[0-9]+" ../.llm-actions/vllm.log)
        vllm serve "${{ inputs.model }}" --max-model-len $len &> ../.llm-actions/vllm.log &

    #########################################
    # 4. Wait for server to be ready
    #########################################

    - name: Wait until ready (30 min max)
      shell: bash
      run: |
        for i in {1..360}; do
          if curl -sSf http://localhost:8000/v1/models > /dev/null; then
            echo "vLLM server is ready"; exit 0
          fi
          sleep 5
        done
        echo "Timeout: server not ready in 30 min"
        cat ../.llm-actions/vllm.log && exit 1

    #########################################
    # 5. Show logs
    #########################################

    - name: Show logs
      id: show_logs
      shell: bash
      run: |
        cat ../.llm-actions/vllm.log
        echo "log<<EOF"              >> "$GITHUB_OUTPUT"
        cat ../.llm-actions/vllm.log >> "$GITHUB_OUTPUT"
        echo "EOF"                   >> "$GITHUB_OUTPUT"
