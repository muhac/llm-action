name: Run LLMs

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  code:
    name: Run Python Code
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model:
          [
            "Qwen/Qwen3-1.7B",
            "google/gemma-2-2b-it",
            "Qwen/Qwen2.5-3B-Instruct",
            "meta-llama/Llama-3.2-3B-Instruct",
            "microsoft/Phi-4-mini-instruct",
            "Qwen/Qwen3-4B",
            "Qwen/Qwen3-14B-AWQ",
          ]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Free Disk Space
        uses: jlumbroso/free-disk-space@main
        with:
          large-packages: false
          swap-storage: false

      - name: Pull docker image
        run: docker pull muhac/jupyter-pytorch:env-2502a

      - name: Test the environment
        run: |
          tree
          docker run -v ./:/srv \
          muhac/jupyter-pytorch:env-2502a \
          bash -i -c "ls -lh /srv"

      - name: Start Docker Container
        run: |
          docker run -d --name container -v ./:/srv \
          muhac/jupyter-pytorch:env-2502a tail -f /dev/null

      - name: Login to Hugging Face
        run: |
          docker exec container bash -i -c "huggingface-cli login --token ${{ secrets.HF }}"
        continue-on-error: true

      - name: Run LLM
        run: |
          docker exec container bash -i -c "python /srv/huggingface.py ${{ matrix.model }} \
          'How many times does the letter R appear in the word strawberry?' \
          '陨石为什么每次都能精准砸到陨石坑？' \
          "

      - name: Show results
        run: cat response.txt

      - name: Stop Docker Container
        run: |
          docker stop container
          docker rm container

  vllm:
    name: Hosted by vLLM
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model:
          [
            "Qwen/Qwen3-1.7B",
            "google/gemma-2-2b-it",
            "Qwen/Qwen2.5-3B-Instruct",
            "meta-llama/Llama-3.2-3B-Instruct",
            "Qwen/Qwen3-4B",
            "Qwen/Qwen3-8B-AWQ",
          ]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Free Disk Space
        uses: jlumbroso/free-disk-space@main
        with:
          large-packages: false
          swap-storage: false

      - name: Checkout vLLM
        uses: actions/checkout@v4
        with:
          repository: "vllm-project/vllm"
          ref: "v0.9.0"

      - name: Install dependencies
        run: |
          sudo apt-get update  -y
          sudo apt-get install -y gcc-12 g++-12 libnuma-dev jq
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

      - name: Build vLLM
        run: |
          pip install --upgrade pip
          pip install "cmake>=3.26" wheel packaging ninja "setuptools-scm>=8" numpy
          pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
          sudo -E VLLM_TARGET_DEVICE=cpu python setup.py install

      - name: Install libraries
        run: pip install openai huggingface_hub[cli]

      - name: Login to Hugging Face
        run: huggingface-cli login --token ${{ secrets.HF }}
        continue-on-error: true

      - name: Run models
        run: vllm serve ${{ matrix.model }} &> vllm.log &

      - name: Check the server status
        id: check_server
        run: |
          for i in {1..30}; do
              if curl -sSf http://localhost:8000/v1/models > /dev/null; then
                exit 0  # Server started
              fi
              if grep -q "ValueError: The model's max seq len" vllm.log; then
                exit 1  # Error detected
              fi
              sleep 10
          done
          cat vllm.log
        continue-on-error: true

      - name: Fix max seq len issue
        if: steps.check_server.outcome != 'success'
        id: check_maxlen
        run: |
          allowed_len=$(grep -m1 -oP "tokens that can be stored in KV cache \\(\\K[0-9]+" vllm.log)
          echo "Max sequence length issue detected, retrying with allowed length: $allowed_len"
          echo "allowed_len=$allowed_len" >> "$GITHUB_OUTPUT"

      - name: Run models (retry)
        if: steps.check_server.outcome != 'success'
        run: vllm serve ${{ matrix.model }} --max-model-len ${{ steps.check_maxlen.outputs.allowed_len }} &> vllm.log &

      - name: Wait for the server to start
        run: |
          for i in {1..360}; do
              if curl -sSf http://localhost:8000/v1/models > /dev/null; then
                  break
              fi
              sleep 5
          done
          if ! curl -sSf http://localhost:8000/v1/models > /dev/null; then
              echo "\nTimeout: Server did not start within 30 minutes."
          fi

      - name: Show logs
        run: cat vllm.log

      - name: Test the server
        run: curl --retry 5 --retry-delay 10 http://localhost:8000/v1/models

      - name: Demo chat
        run: |
          curl http://localhost:8000/v1/chat/completions \
          -o response.json \
          -H "Content-Type: application/json" \
          -d '{
              "model": "${{ matrix.model }}",
              "messages": [
                  {"role": "system", "content": "You are a teacher."},
                  {"role": "user", "content": "How many times does the letter R appear in the word strawberry?"}
              ]
          }'

      - name: Show response
        run: cat response.json | jq . || cat response.json

      - name: Demo chat (Chinese)
        run: |
          curl http://localhost:8000/v1/chat/completions \
          -o response.json \
          -H "Content-Type: application/json" \
          -d '{
              "model": "${{ matrix.model }}",
              "messages": [
                  {"role": "user", "content": "陨石为什么每次都能精准砸到陨石坑？"}
              ]
          }'

      - name: Show response
        run: cat response.json | jq . || cat response.json
