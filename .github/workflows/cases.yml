name: Run LLMs

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  code:
    name: Run Python Code
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model:
          [
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
            "google/gemma-2-2b-it",
            "Qwen/Qwen2.5-3B-Instruct",
            "meta-llama/Llama-3.2-3B-Instruct",
            "microsoft/Phi-4-mini-instruct",
          ]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Pull docker image
        run: docker pull muhac/jupyter-pytorch:env-2502a

      - name: Test the environment
        run: |
          tree
          docker run -v ./:/srv \
          muhac/jupyter-pytorch:env-2502a \
          bash -i -c "ls -lh /srv"

      - name: Start Docker Container
        run: |
          docker run -d --name container -v ./:/srv \
          muhac/jupyter-pytorch:env-2502a tail -f /dev/null

      - name: Login to Hugging Face
        run: |
          docker exec container bash -i -c "huggingface-cli login --token ${{ secrets.HF }}"
        continue-on-error: true

      - name: Run LLM
        run: |
          docker exec container bash -i -c "python /srv/huggingface.py ${{ matrix.model }} \
          'What is GitHub Actions?' \
          '这不显得您枪法准是什么梗？' \
          "

      - name: Show results
        run: cat response.txt

      - name: Stop Docker Container
        run: |
          docker stop container
          docker rm container

  vllm:
    name: Hosted by vLLM
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model:
          [
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
            "google/gemma-2-2b-it",
            "Qwen/Qwen2.5-3B-Instruct",
            "meta-llama/Llama-3.2-3B-Instruct",
          # "microsoft/Phi-4-mini-instruct",
          ]
    env:
      LARGE_MODELS: '["meta-llama/Llama-3.2-3B-Instruct", "microsoft/Phi-4-mini-instruct"]'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Checkout vLLM
        uses: actions/checkout@v4
        with:
          repository: "vllm-project/vllm"
          ref: "v0.7.2"

      - name: Install dependencies
        run: |
          sudo apt-get update  -y
          sudo apt-get install -y gcc-12 g++-12 libnuma-dev jq
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

      - name: Build vLLM
        run: |
          pip install --upgrade pip
          pip install "cmake>=3.26" wheel packaging ninja "setuptools-scm>=8" numpy
          pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
          sudo -E VLLM_TARGET_DEVICE=cpu python setup.py install

      - name: Install libraries
        run: pip install openai huggingface_hub[cli]

      - name: Login to Hugging Face
        run: huggingface-cli login --token ${{ secrets.HF }}
        continue-on-error: true

      - name: Run models
        if: ${{ !contains(fromJSON(env.LARGE_MODELS), matrix.model) }}
        run: vllm serve ${{ matrix.model }} &> vllm.log &

      - name: Run models (Large)
        if: ${{ contains(fromJSON(env.LARGE_MODELS), matrix.model) }}
        run: vllm serve ${{ matrix.model }} --max-model-len 37440 &> vllm.log &

      - name: Wait for the server
        run: |
          sleep 60   # for debugging, small models can be run with ~40s
          cat vllm.log
          while ! grep -q "running on http://0.0.0.0:8000" vllm.log; do
              sleep 1
          done

      - name: Show logs
        run: cat vllm.log

      - name: Test the server
        run: curl --retry 5 --retry-delay 10 http://localhost:8000/v1/models

      - name: Demo chat
        run: |
          curl http://localhost:8000/v1/chat/completions \
          -o response.json \
          -H "Content-Type: application/json" \
          -d '{
              "model": "${{ matrix.model }}",
              "messages": [
                  {"role": "system", "content": "You are a technical expert."},
                  {"role": "user", "content": "What is GitHub Actions?"}
              ]
          }'

      - name: Show response
        run: cat response.json | jq . || cat response.json

      - name: Demo chat (Chinese)
        run: |
          curl http://localhost:8000/v1/chat/completions \
          -o response.json \
          -H "Content-Type: application/json" \
          -d '{
              "model": "${{ matrix.model }}",
              "messages": [
                  {"role": "user", "content": "这不显得您枪法准是什么梗？"}
              ]
          }'

      - name: Show response
        run: cat response.json | jq . || cat response.json
